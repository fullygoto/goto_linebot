from flask import Flask, request
import os
import requests
from openai import OpenAI
import PyPDF2
import chromadb
from chromadb.utils import embedding_functions
import glob
from bs4 import BeautifulSoup

app = Flask(__name__)

LINE_CHANNEL_ACCESS_TOKEN = os.environ.get("LINE_CHANNEL_ACCESS_TOKEN")
OPENAI_API_KEY = os.environ.get("OPENAI_API_KEY")
DATA_DIR = "data"

ef = embedding_functions.OpenAIEmbeddingFunction(
    api_key=OPENAI_API_KEY,
    model_name="text-embedding-3-small"
)

client = chromadb.PersistentClient(path="./chroma_db")
collection = client.get_or_create_collection(name="goto_kanko", embedding_function=ef)

def split_text_paragraphs(text, window=10, step=5):
    lines = text.split('\n')
    chunks = []
    for i in range(0, len(lines), step):
        chunk = "\n".join(lines[i:i+window])
        if len(chunk.strip()) >= 30:
            chunks.append(chunk)
    return chunks

def load_docs_to_db():
    for filepath in glob.glob(os.path.join(DATA_DIR, "*.pdf")):
        with open(filepath, "rb") as f:
            reader = PyPDF2.PdfReader(f)
            for i, page in enumerate(reader.pages):
                text = page.extract_text()
                if not text: continue
                chunks = split_text_paragraphs(text)
                for idx, chunk in enumerate(chunks):
                    docid = f"{os.path.basename(filepath)}-p{i}-c{idx}"
                    collection.add(
                        documents=[chunk],
                        metadatas=[{"file": os.path.basename(filepath), "page": i, "chunk": idx}],
                        ids=[docid]
                    )
    for filepath in glob.glob(os.path.join(DATA_DIR, "*.txt")):
        with open(filepath, "r", encoding="utf-8") as f:
            text = f.read()
        chunks = split_text_paragraphs(text)
        for idx, chunk in enumerate(chunks):
            docid = f"{os.path.basename(filepath)}-c{idx}"
            collection.add(
                documents=[chunk],
                metadatas=[{"file": os.path.basename(filepath), "chunk": idx}],
                ids=[docid]
            )
    print("å†æŠ•å…¥å¾Œãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•°:", collection.count())

if collection.count() == 0:
    load_docs_to_db()

def search_paragraph(user_message):
    title_query = user_message.replace("ã«ã¤ã„ã¦", "").replace("ã‚’æ•™ãˆã¦", "")
    res = collection.get(where_document={"$contains": title_query})
    if res and res['documents']:
        return res['documents'][0]
    search_res = collection.query(query_texts=[user_message], n_results=1)
    if search_res and search_res['documents'][0]:
        return search_res['documents'][0][0]
    return ""

def get_kyusho_ferry_status():
    url = "https://kyusho.co.jp/status"
    try:
        res = requests.get(url, timeout=8)
        soup = BeautifulSoup(res.content, "html.parser")
        ferry_sections = soup.find_all("section", class_="statusBox")
        for section in ferry_sections:
            title = section.find("h3")
            if title and "é•·å´ï½äº”å³¶èˆªè·¯" in title.get_text():
                detail = section.find("div", class_="statusDetail")
                if detail:
                    return detail.get_text(separator='\n', strip=True)
        return "ä¹å·å•†èˆ¹ é•·å´ï½äº”å³¶èˆªè·¯ã®é‹è¡ŒçŠ¶æ³ã‚’å–å¾—ã§ãã¾ã›ã‚“ã§ã—ãŸã€‚"
    except Exception as e:
        print("ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°ã‚¨ãƒ©ãƒ¼:", e)
        return "é‹è¡Œæƒ…å ±ã®å–å¾—ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"

def generate_answer(user_message):
    if ("ä¹å·å•†èˆ¹" in user_message) or ("äº”å³¶èˆªè·¯" in user_message) or \
       ("é•·å´" in user_message and "é‹èˆª" in user_message):
        return get_kyusho_ferry_status()
    related_text = search_paragraph(user_message)
    if not related_text or len(related_text) < 10:
        return "ã™ã¿ã¾ã›ã‚“ã€ãã®ä»¶ã«ã¤ã„ã¦ã¯ç¾åœ¨ã®æƒ…å ±ã§ã¯ãŠç­”ãˆã§ãã¾ã›ã‚“ã€‚"
    prompt = (
        "ã‚ãªãŸã¯äº”å³¶ã®è¦³å…‰å…¬å¼AIã§ã™ã€‚ä»¥ä¸‹ã®å‚è€ƒæƒ…å ±ã ã‘ã‚’æ ¹æ‹ ã«ã€äº‹å®Ÿã®ã¿æ­£ç¢ºã«ç­”ãˆã¦ãã ã•ã„ã€‚"
        "æ ¹æ‹ ãŒä¸ååˆ†ãªå ´åˆã¯ã€Œæƒ…å ±ãŒã‚ã‚Šã¾ã›ã‚“ã€ã¨è¿”ã—ã¦ãã ã•ã„ã€‚\n\n"
        f"ã€å‚è€ƒæƒ…å ±ã€‘\n{related_text}"
    )
    try:
        client = OpenAI(api_key=OPENAI_API_KEY)
        response = client.chat.completions.create(
            model="gpt-4o",
            messages=[
                {"role": "system", "content": prompt},
                {"role": "user", "content": user_message}
            ]
        )
        return response.choices[0].message.content.strip()
    except Exception as e:
        print("OpenAI APIã‚¨ãƒ©ãƒ¼:", e, flush=True)
        return "AIã«ã‚ˆã‚‹æ¡ˆå†…æ–‡ç”Ÿæˆä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸã€‚"

@app.route("/webhook", methods=["POST"])
def webhook():
    body = request.json
    events = body.get("events", [])
    for event in events:
        if event["type"] == "message" and event["message"]["type"] == "text":
            user_message = event["message"]["text"]
            reply_token = event["replyToken"]
            reply_text = generate_answer(user_message)
            headers = {
                "Content-Type": "application/json",
                "Authorization": f"Bearer {LINE_CHANNEL_ACCESS_TOKEN}"
            }
            data = {
                "replyToken": reply_token,
                "messages": [{
                    "type": "text",
                    "text": reply_text
                }]
            }
            requests.post("https://api.line.me/v2/bot/message/reply", headers=headers, json=data)
    return "OK"

# ğŸ”„ ãƒ‡ãƒ¼ã‚¿ãƒ™ãƒ¼ã‚¹å†æŠ•å…¥ç”¨API
@app.route("/reload", methods=["POST"])
def reload_db():
    try:
        collection.delete()
        load_docs_to_db()
        return "DBãƒªãƒ­ãƒ¼ãƒ‰å®Œäº†"
    except Exception as e:
        print("ãƒªãƒ­ãƒ¼ãƒ‰ã‚¨ãƒ©ãƒ¼:", e)
        return f"ãƒªãƒ­ãƒ¼ãƒ‰å¤±æ•—: {e}"

if __name__ == "__main__":
    port = int(os.environ.get("PORT", 10000))
    app.run(host="0.0.0.0", port=port)
